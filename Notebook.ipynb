{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Techniques for Network Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authored by: Neel Kanwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # importing libraries for to exploit dataframe and plotting operations on provided libraries\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using log tcp labeled data as a dataframe where data is classified based on spaces, we used chunks in the beginning to simply the data loading operation for machine learning then applied it to whole log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"log_tcp_complete_classes.txt\", sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading header of data on dataframe. We could also use head without braces to see in unscrolable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#31#c_ip:1</th>\n",
       "      <th>c_port:2</th>\n",
       "      <th>c_pkts_all:3</th>\n",
       "      <th>c_rst_cnt:4</th>\n",
       "      <th>c_ack_cnt:5</th>\n",
       "      <th>c_ack_cnt_p:6</th>\n",
       "      <th>c_bytes_uniq:7</th>\n",
       "      <th>c_pkts_data:8</th>\n",
       "      <th>c_bytes_all:9</th>\n",
       "      <th>c_pkts_retx:10</th>\n",
       "      <th>...</th>\n",
       "      <th>s_pkts_data_std:198</th>\n",
       "      <th>c_seg_cnt:199</th>\n",
       "      <th>c_sit_avg:200</th>\n",
       "      <th>c_sit_std:201</th>\n",
       "      <th>s_seg_cnt:202</th>\n",
       "      <th>s_sit_avg:203</th>\n",
       "      <th>s_sit_std:204</th>\n",
       "      <th>c_pkts_push:205</th>\n",
       "      <th>s_pkts_push:206</th>\n",
       "      <th>class:207</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246.25.63.48</td>\n",
       "      <td>60119</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>12.589583</td>\n",
       "      <td>19.177348</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>class:google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246.216.210.193</td>\n",
       "      <td>48696</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>class:google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246.216.210.193</td>\n",
       "      <td>57238</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>class:google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246.25.147.110</td>\n",
       "      <td>43053</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>167</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>9.830455</td>\n",
       "      <td>33.152877</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>class:google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254.211.188.176</td>\n",
       "      <td>52660</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>1</td>\n",
       "      <td>439</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>113.991744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>11.591938</td>\n",
       "      <td>17.092896</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>class:google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        #31#c_ip:1  c_port:2  c_pkts_all:3  c_rst_cnt:4  c_ack_cnt:5  \\\n",
       "0     246.25.63.48     60119            21            1           19   \n",
       "1  246.216.210.193     48696             6            0            5   \n",
       "2  246.216.210.193     57238             6            0            5   \n",
       "3   246.25.147.110     43053            20            1           18   \n",
       "4  254.211.188.176     52660            20            0           19   \n",
       "\n",
       "   c_ack_cnt_p:6  c_bytes_uniq:7  c_pkts_data:8  c_bytes_all:9  \\\n",
       "0             17             167              1            167   \n",
       "1              3             200              1            200   \n",
       "2              3             200              1            200   \n",
       "3             16             167              1            167   \n",
       "4             17             439              1            439   \n",
       "\n",
       "   c_pkts_retx:10      ...       s_pkts_data_std:198  c_seg_cnt:199  \\\n",
       "0               0      ...                  0.000000              0   \n",
       "1               0      ...                  0.000000              0   \n",
       "2               0      ...                  0.000000              0   \n",
       "3               0      ...                  0.000000              0   \n",
       "4               0      ...                113.991744              0   \n",
       "\n",
       "   c_sit_avg:200  c_sit_std:201 s_seg_cnt:202  s_sit_avg:203  s_sit_std:204  \\\n",
       "0            0.0            0.0            25      12.589583      19.177348   \n",
       "1            0.0            0.0             0       0.000000       0.000000   \n",
       "2            0.0            0.0             0       0.000000       0.000000   \n",
       "3            0.0            0.0            34       9.830455      33.152877   \n",
       "4            0.0            0.0            17      11.591938      17.092896   \n",
       "\n",
       "   c_pkts_push:205  s_pkts_push:206     class:207  \n",
       "0                1                2  class:google  \n",
       "1                1                1  class:google  \n",
       "2                1                1  class:google  \n",
       "3                1                0  class:google  \n",
       "4                1                1  class:google  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the columns to understand the categorical and numerical features for our machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['#31#c_ip:1', 'c_port:2', 'c_pkts_all:3', 'c_rst_cnt:4', 'c_ack_cnt:5',\n",
       "       'c_ack_cnt_p:6', 'c_bytes_uniq:7', 'c_pkts_data:8', 'c_bytes_all:9',\n",
       "       'c_pkts_retx:10',\n",
       "       ...\n",
       "       's_pkts_data_std:198', 'c_seg_cnt:199', 'c_sit_avg:200',\n",
       "       'c_sit_std:201', 's_seg_cnt:202', 's_sit_avg:203', 's_sit_std:204',\n",
       "       'c_pkts_push:205', 's_pkts_push:206', 'class:207'],\n",
       "      dtype='object', length=207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing features based on distinction in difference labeled classes, we may choose these features based on any logical or numerical order and It is quite obserable the data classified by youtube has higher packet size than data travelling to google. Some of the futures are chosen based on different values in all classes such as 's_seg_cnt:202'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols= np.arange(173,165).tolist() + np.arange(141,150).tolist() + np.arange(153,162).tolist() \\\n",
    "+ np.arange(131,140).tolist() + np.arange(174,182).tolist() + np.arange(183,191).tolist()+[41,200,204,9, 2,5,6, 16, 30, 45, 52];\n",
    "categorical_features = ['s_sit_avg:203']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.The categorical value represents the numerical value of the entry in the dataset. IN OneHotEncoder we put sparse equal to false so that it can return a matrix not an array. We use first column of categorical feature used for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 1.0000000e+00, ..., 1.0051780e+03,\n",
       "        1.1925000e+01, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 5.6728460e+03,\n",
       "        1.2197000e+01, 2.2559000e+01],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 5.6743050e+03,\n",
       "        1.1973000e+01, 2.2740000e+01],\n",
       "       ...,\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 3.0437000e+01,\n",
       "        3.0437000e+01, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 3.0565000e+01,\n",
       "        3.0565000e+01, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.0795599e+04,\n",
       "        3.0161000e+01, 2.2382000e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False,categorical_features=[0])\n",
    "# using the return value of one hot encoder for fitting over selected features\n",
    "X_trans = ohe.fit_transform(df.iloc[:, feature_cols])\n",
    "X_trans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the parameters of transformed encoder. It can be seen that for unknown catergorical feature we ignore the stuff. n_values is auto which means it will determine the range from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical_features': [0],\n",
       " 'dtype': numpy.float64,\n",
       " 'handle_unknown': 'error',\n",
       " 'n_values': 'auto',\n",
       " 'sparse': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_names=ohe.get_params(deep=True)\n",
    "feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['class:207']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are inputing encoded data to split for training in testing. As theoretically we can do it either 60/40 or 75/25. Default value that function takes is 0.25 for test size. Seed to randomize the data is 1 and we are straifying data based on labeled class in column 206. This help us to utilize better the supervized learning algorithms for our labeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 695) (25000, 695)\n",
      "(75000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.25, random_state=1, stratify=y)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of a dataset is a common requirement for many machine learning estimators. They might behave badly if the individual features do not more or less look like standard normally distributed data. So this removes mean and scales it to unit variance which will be used to observe the validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_tr_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a bagging technique for both classification and regression. The general concept is that you divide your data into several portions, use a relatively weak classifier/regressor to process, and then combine them. Firstly we wanted to use SVM. SVM is a type of supervised machine learning classification algorithm. In our case where data is dimensionalized and labeled.we can try a  typical machine learning algorithm tries to find a boundary that divides the data in such a way that the misclassification error can be minimized. Theoriticaly SVM finds an optimal decision boundry. \n",
    "SVC(Kernel='linear') and LinearSVC() are typically same algorithm implemented in different way as described by Sckilearn documentation. \n",
    "\n",
    "Random Forest here performs faster and is much computationaly cheaper than SVM and NN.In recent years, tress as being used for forescasting which also avoids short comings of traditional decision trees. In our case data is aligned so Random forest performs faster and develops higher accuracy on testing data. This algorithm can further be improved if we divide our training data in testing and validation.\n",
    "\n",
    "N_estimator parameter help to identify number of tress which is 100 in our case, intger multiple of 10  because we have 10 classes in our data. Tree performs well with depth 20, because we have many feature in a singles class so this is expanded until all leaves are pure or until all leaves contain less than minimum samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   51.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=20,random_state=0,verbose=1)\n",
    "\n",
    "model = clf.fit(X_tr_st, y_train) # fitting with standarized data.\n",
    "predict = model.predict(X_test_st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is accuracy score is pretty well and can be increased on the cost of heavy computation by increasing the estimator or depth on classifier. Max_feature parameter can be used to further explicit and divide feature to converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95348\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, predict) # measuring accuracy for test data. \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use find the unique classes name for confusion matrix to see probablistic classificaiton of all objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class:google', 'class:bing', 'class:linkedin', 'class:instagram',\n",
       "       'class:youtube', 'class:netflix', 'class:spotify',\n",
       "       'class:facebook', 'class:ebay', 'class:amazon'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels=y.unique()\n",
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot show the distribution of classes on the probablisitc plane. The labels are in correspondance with numbers from 0 to 9. Some classes have higher predictability than other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAEQCAYAAAA9C4aXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XncHFWd7/HPNwk7UcAAQhIWIaCoV5YICC4ICgHZvFccUBYdNDMKLoOjAuNcGLzOMCOXATecCAjIJoKMucoIiDK4sIVFFgEJCCQkbGHfyfP87h/nNFSaXurJU5Xufvi+X696PU9XnT51qrr616fOOVWliMDMzFob1+sCmJn1MwdJM7MOHCTNzDpwkDQz68BB0sysAwdJM7MOehokJa0k6f9JekLST0aRz8clXVJl2XpF0nsk3VFDviPe15Iul/SpqstSN0k7SJrf63L0K0kbSApJE2rK/0hJJxdef1jSPElPS9pC0q2Sdqhj3XUoFSQlfUzSnLyRCyX9l6R3V7D+jwBrA2+IiH2WNpOIOCsidq6gPLXKB+bGndJExG8jYtMaVt9xX0s6WtKZNay3kf89kp7Lx9ADkk6TtGpd66tL03Y0pnWX4fpLBThJm0j6iaRH8g/jTZIOkzS+7jJGxD9HRPHH9Tjg0IhYNSJuiIi3RsTldZejKl2DpKTDgBOAfyZ9ydYDvgfsVcH61wf+HBGLK8hr4NX1y571w77eIyJWBTYHtgCO6GFZRmOP/IVvTAtG8uaaP2ckbQRcDcwD3h4Rrwf2AaYDE+tcdxvrA7eONpO691tbEdF2Al4PPA3s0yHNCqQguiBPJwAr5GU7APOBLwEPAQuBT+Zl/wS8CLyU13EwcDRwZiHvDYAAJuTXnwDuBp4C/gJ8vDD/d4X3bQdcCzyR/25XWHY58HXg9zmfS4BJbbatUf6vFMq/N7Ab8GfgUeDIQvqtgSuBx3Pa7wDL52VX5G15Jm/vXxXy/yrwAPCjxrz8no3yOrbMr9cFHgF2aFPet+Tte5x0UO7Zbl83vW9G0/I/ltlXwLbAH/L6/tiuXDntPcAHCq//DfhF4fWHgBuAJ0lf7qNbHAcHAfflffAPheUrAacBjwF/Ar7c2Ied9ktedhrpR/+/8rb/Hngj6Th+DLgd2KLddjRt4545/8fz+t7S9L6vAjcBLwAT8ud5AfAw6Xj+fNOxNCfvjweB4/P8+/K+eDpP72pRjjOL+7bF8sb+bHyvPgnclj/ju4G/KaSdBPw8b9OjwG+BcXnZV4H78/vuAHbK84/OZVghl7Fx3N/VvA9JFbXDgbuARcB5wBpN5Tw4b/cVneJVXVO3IDkDWNzYmW3SHANcBawFrEn60ny9EGQW5zTLkYLLs8DqxZ1ZyKv59csfJrBKPmA2zcvWAd7aHCSBNUgH9wH5ffvl128ofPHvAjYhfbkuB47tECQXA/87l//TpAP6bNIv8luB54E35fRbkQLHhFz224AvFvILYOMW+f9rPqBWohAkc5pP53xWBi4GjmtT1uWAucCRwPLAjqSDd9NW+7bF+1+1vNO+AiaTDurdSAf6B/PrNbsFSWAKcDNwYtO+eHvO63+QAsPeTcfBD3I53kEKNG/Jy48lfXnXAKYCt/DKD023/XIaKehuBawI/JoUsA4ExgP/B/hNtyCZ99EzeT8sR/phncsrP5L3ADfm8q2Ut/M60rG1PPAmUoDaJae/Ejgg/78qsG2rANdmXz9AroyUDJIfIv0gC3gf6Tva+GH+F+D7eZuWA96T021K+jFbt5DnRm2+x83HffFY+CIpfkwhfQf+AzinqZxnkL7/K/VjkPw48ECXNHcBuxVe7wLcUzjwnyt+oKQaWeMDb96Zza9f/jDzTnoc+F/NO4slg+QBwDVNy68EPlH44n+tsOyzwC/bbFuj/OPz64m5PNsU0lxH/jK3eP8XgQs7HCw7kGpwKzbNm9+Uz2xSULmJXEtvsa73kL4c4wrzziHXyJr3bYv3v2p5p31FqkX8qCn9xcBBbfK/h1SreCrvh8uA1TqU5wTg35uOgymF5dcA++b/7wZmFJbN5JUg2W2/nAb8oLDsc8BthddvBx5vsR2P5+k/8/x/BM4rpBtHqmXtUHjfXxeWbwPc17TNRwA/zP9fQToDmNSUprEvOgXJl4r7o8XyjnkA/wl8If9/DPAzCsdtnr8x6bv8AWC5TscSnYPkbeQaaH69Ti5/o6IR5EpIr6ZubZKLgEld2gLWBe4tvL43z3s5j1iyHexZ0i/jiETEM6RT1L8FFkr6haQ3lyhPo0yTC68fGEF5FkXEUP7/ufz3wcLy5xrvz43lP88dE0+S2nEndcgb4OGIeL5Lmh8AbwO+HREvtEmzLjAvIoYL85q3e2m021frA/tIerwxAe8mHeTt7B0RE0k/BG+msG8kbSPpN5IelvQE6XNu3nftyrIuqVbTUPz8y+yX5s+z5efbtB2r5WnvwnpeXm9e37ym9RTLuD6wbtP+O5LU7g/pFHMT4HZJ10ranfIW0flzWIKkXSVdJenRXI7deGXff5NUI75E0t2SDs/bN5dUCTgaeEjSuUvZgbU+cGFhH9wGDPHKfoAl99sy1y1IXkk6ndy7Q5oFpA1tWC/PWxrPkE4rG95YXBgRF0fEB0kHwO2k4NGtPI0y3b+UZRqJk0jlmhYRryMd9Orynui0MPcAnwCcAhwtaY02SRcAUyUVP9ORbHfHcrQwj1STXK0wrRIRx3ZdUcR/k2pwxxVmn02qMU+N1NHwfbrvu4aFpNPYhvUK/492v5S1xHEnSblMxfUU9/E84C9N+29iROwGEBF3RsR+pGasfwXOl7QK5T6nX5HOuLqStAKpXfQ4YO2IWA24iLzvI+KpiPhSRLwJ2AM4TNJOednZEfHuvN2RyzlS84Bdm/bDihHRbr8tcx2DZEQ8QWoz+a6kvSWtLGm5/MvzbznZOcDXJK0paVJOv7RDSW4E3itpPUmvp9D7KWltSXvmA+UF0inPUIs8LgI2ycOWJkj6K2AzUuNz3SaS2k2fzrXczzQtf5DU9jQSJwLXRRpS8QtS8GjlatKPzFfyZ7QD6aA+t+R6HgQ2aAomnZwJ7CFpF0njJa2YxydOKfn+E4APSto8v54IPBoRz0vaGvhYyXwgNfYfIWn1vP7PFZaNdr+MpAwfkrSTpOVInZUvkNroW7kGeFLSV/MY1vGS3ibpnQCS9pe0Zq6RPp7fM0RqEx+m83F0FLCdpG9KemPOb2NJZ0parSnt8qS2wIeBxZJ2BV4eTidp9/xekY7tIWBI0qaSdsxB9nlSjbvV97Gb7wPfkLR+Xt+akqoYOVOZrl+IiDgeOAz4GmlHzgMOJbVbQGrYnkNqL7sZuD7PG7GIuBT4cc7rOpYMbONIB94CUi/b+0htZM15LAJ2z2kXkRrQd4+IR5amTCP096Qv91OkWu6Pm5YfDZyeTy0+2i2zfLDMIJ16QvoctpT08ea0EfEiqXd1V1JHxPeAAyPi9pJlbwwwXyTp+m6JI2IeaRjYkbxyXHyZkmNvI+JhUoP8P+ZZnwWOkfQU6Yf2vJLlhtR2dy+pw+US0iiBxnpGu19KiYg7gP2Bb+f17EEaKvRim/RDOc3mudyPACeTRpRA+txvlfQ06Ydy34h4PiKeBb4B/D4fR9u2yPsu4F2kNr1bc/PFBaTv6VNNaZ8CPk/a34+Rjt/ZhSTTSDXTp0lnlt+LNMZxBVKH2SOkZpC1SMfCSJ2Y13dJ/uyvIrXX9g3lxtIxR9IM0gcwHji5zGlgr0iaSgoYbyTVEmZFxIm9LVVneVDyHOD+iBhJe9kyl2tPJ5PadYPUgXJlb0vVmqS/Az5FKufNpF7qbm3WVqMxee12/gJ/l1R72AzYT9JmvS1VR4uBL0XEW0hDiA7p8/ICfIHUyD4ITiT1yr+ZNHyoL8staTKpVjc9It5G+oHft7elsjEZJEkDcedGxN35dOdcqrlCqBYRsTAirs//P0X6Eo+2V7o2ud3vQ6TaWV+T9DrgvaSOLyLixYh4vPO7emoCsFIeUbIyS98JahUZq0FyMksOG5hPHwedIkkbkC7Zu7q3JenoBFJb73C3hH3gTaQ20x9KukHSybnzr+/kHt3jSFeXLASeiIgxceOWQTZWg2SroSN93/iah/tcQLpK58lel6eVPF7voYi4rtdlKWkCsCVwUkRsQerpPry3RWpN0uqkM54NSeMuV5G0f29LZWM1SM5nyXFzU+jz05Y8bOQC4KyI+Gmvy9PB9sCeku4hNWPsqBrvHlSB+aSrbxo18/NJQbMffYA0dvLhiHgJ+CnpPgTWQ2M1SF4LTJO0oaTlSY3fs7u8p2fyGLRTSJfDHd/r8nQSEUdExJSI2IC0X38dEX1b24mIB4B5khq3n9uJdBOMfnQfsG0ejyxSWfuyk+m1pDe3HqpZRCyWdCjpWuLxwKkRMepbNdVoe9I15zdLujHPOzIiLuphmcaSzwFn5R/Mu0l3vek7EXG1pPNJY40Xk+6KNKu3pbIxO07SzKwKY/V028ysEg6SZmYdOEiamXXgIGlm1sGYD5KSZva6DGUNUllhsMo7SGWFwSvvWDbmgyTpVv6DYpDKCoNV3kEqKwxeeces10KQNDNbagM5TnLSGuNj/anlxsE/vGiINd9Q7nnsd97U2/sevMQLLMcKPS3DSAxSeUdc1rIPjhiJEXzVer1vn+cZXowXRrUXdnn/KrHo0XI3K7/uphcujogZo1lfXQbyipv1p07gD7+s/qY+u0/eqvI8rWbjyv0AjpTGVR8lY/Hi7on6xNVx2ajzWPToENdcvF73hMD4de7s9sC8nhnIIGlm/S+A4YG4m15nDpJmVosgeCmW5tlg/cVB0sxq45qkmVkbQTA0gB3DzRwkzaw2w/3/QICuHCTNrBYBDI2BINkXg8klzZB0h6S5kvry+SNmNnLDRKmpn/W8Jll4RvYHSc8juVbS7Ijo11vsm1kJAbw0Btok+6EmOVDPyDazcoJgqOTUz/ohSJZ6RrakmZLmSJrz8KLBH3tlNuYFDJWc+lk/BMlSz8iOiFkRMT0ippe9FtvMeiddcVNu6mc9b5NkAJ+RbWZliKFa7hSybPVDkHz5GdnA/aRnOX+st0Uys9FKHTcOkqM2gM/INrMS0jhJB8lKRMRFwEW9LoeZVWvYNUkzs9ZckzQz6yAQQ30xgGZ0HCTNrDY+3TYzayMQL8bgj2l2kDSzWqTB5D7d7ok7b1qF3adMrzzfixfcUHmeALusu3kt+dZCNZ0e1XWjg+F6LlGNfr8MZEC448bMrI0IMRSuSZqZtTXsmqSZWWup42bwQ8zgb4GZ9SV33JiZdTHkcZJmZq35ihszsy6G3bttZtZausGFg6SZWUuBeMmXJZqZtRaBB5ObmbUnDyY3M2sncE3SzKwjd9yYmbURaEzcdHfww7yZ9aX0SNkJpaZuJE2V9BtJt0m6VdIX8vw1JF0q6c78d/U8X5K+JWmupJskbVnI66Cc/k5JB3Vbt4OkmdVEDJWcSlgMfCki3gJsCxwiaTPgcOCyiJgGXJZfA+wKTMvTTOAkSEEVOArYBtgaOKoRWNtxkDSzWgTpipsyU9e8IhZGxPX5/6eA24DJwF7A6TnZ6cDe+f+9gDMiuQpYTdI6wC7ApRHxaEQ8BlwKzOi0brdJmlltRnBn8kmS5hRez4qIWa0SStoA2AK4Glg7IhZCCqSS1srJJgPzCm+bn+e1m9+Wg6SZ1SJCI7l2+5GI6PpMFkmrAhcAX4yIJ9X+cSOtFkSH+W35dNvMapE6bsaXmsqQtBwpQJ4VET/Nsx/Mp9Hkvw/l+fOBqYW3TwEWdJjfloOkmdUkPeOmzNQ1p1RlPAW4LSKOLyyaDTR6qA8CflaYf2Du5d4WeCKfll8M7Cxp9dxhs3Oe19bgnm6r+vi+y5StKs8T4Lz5v6s8z49O3a7yPGtV11MYazgOgNqewvhakjpuKvvctwcOAG6WdGOedyRwLHCepIOB+4B98rKLgN2AucCzwCcBIuJRSV8Hrs3pjomIRzuteHCDpJn1vaquuImI39G6PRFgpxbpAzikTV6nAqeWXbeDpJnVYqxcceMgaWa18YPAzMzaiICXhh0kzcxaSqfbDpJmZm2N4IqbvtXzMN/u7h5mNtgaQ4DKTP2sH2qSjbt7XC9pInCdpEsj4k+9LpiZjYZPtyuRR8E3LlB/SlLj7h4OkmYDzs+4qVjT3T3MbICl3m0/UrYyzXf3aLF8JunmmazIysu4dGY2Uh5MXqE2d/dYQr633CyA12mNjrc2MrP+4NPtCnS4u4eZDbCKb3DRM/3Q9dS4u8eOkm7M0269LpSZjV5Vj2/opZ7XJLvc3cPMBlSEWNznAbCMngdJMxu7xsLptoOkmdVirLRJOkiaWW0cJM3M2vA4STOzLjxOspfqeFBTTQ+r+uiUd1We54/mVf9wMYAD1nt3LfmOW7meq6SGn322lnwHSh3HbQWXa0TAYt9018ysPZ9um5m14TZJM7MuwkHSzKw9d9yYmbUR4TZJM7MOxJB7t83M2nObpJlZG75228ysk0jtkoPOQdLMauPebTOzNsIdN2Zmnfl028ysA/dum5m1EeEgaWbW0VgYAjT4rapm1rciyk3dSDpV0kOSbinMO1rS/a0eRS3pCElzJd0haZfC/Bl53lxJh5fZBtckzawWgRiurnf7NOA7wBlN8/89Io4rzpC0GbAv8FZgXeBXkjbJi78LfBCYD1wraXZE/KnTih0kzaw2VXVuR8QVkjYomXwv4NyIeAH4i6S5wNZ52dyIuBtA0rk5bccg6dNtM6tH7rgpMwGTJM0pTDNLruVQSTfl0/HV87zJwLxCmvl5Xrv5HTlImll9ouQEj0TE9MI0q0TuJwEbAZsDC4H/m+e36i2KDvM78um2mdWmziFAEfFg439JPwB+nl/OB6YWkk4BFuT/281vy0GyQOPH15JvDFX/ZMe6nmp4yr2/rSXfg2sqr8H4iRMrz1NPj/4kM4Dh4fqCpKR1ImJhfvlhoNHzPRs4W9LxpI6bacA1pJrkNEkbAveTOnc+1m09DpJmVo8AKqpJSjoH2IHUdjkfOArYQdLmeU33AH8DEBG3SjqP1CGzGDgkIoZyPocCFwPjgVMj4tZu63aQNLPaVHXtdkTs12L2KR3SfwP4Rov5FwEXjWTdDpJmVh/f4MLMrB352m0zs45ckzQzayMgauzdXlYcJM2sRoMfJPvmihtJ4yXdIOnn3VOb2UAof8VN3+qbIAl8Abit14Uwswo5SFZD0hTgQ8DJvS6LmVWkMZi8zNTH+qVN8gTgK0Db66vyXUFmAqzIysuoWGY2GmPhQWCV1yQlrTDC9LsDD0XEdZ3SRcSsxh1ClmNEqzCzXhlWuamPVRYkJW0t6Wbgzvz6HZK+XeKt2wN7SroHOBfYUdKZVZXLzHpHUW7qZ1XWJL8F7A4sAoiIPwLv7/amiDgiIqZExAaku3L8OiL2r7BcZtYLZTtt+jxIVtkmOS4i7pWWqDpXf48wMxsQ/d8pU0aVQXKepK2BkDQe+Bzw55FkEBGXA5dXWCYz66U+ryWWUWWQ/AzplHs94EHgV3memb1WDfe6AKNXWZCMiIdIbYpmZpXedLeXKguS+RkTr6pcR0TZp56Z2RjT7z3XZVR5uv2rwv8rkp45Ma9NWjN7LXCQfEVE/Lj4WtKPgEuryt/MrBfqvCxxQ2D9GvOvXCxe3OsilKbllq8l34PXf08t+c6+/5pa8t1zyta15FvL9XSqp31u6MknK88zopoeF59uF0h6jFcq1+OAR4HDq8rfzAZM0PeXHJZRSZBUGkH+DtKzbAGGI8bCpe1mNipjIApUclliDogXRsRQnsbArjGz0fK120u6RtKWFeZnZoPO126DpAkRsRh4N/BpSXcBz5AebhER4cBp9lrV5wGwjCraJK8BtgT2riAvMxsjBuFUuowqgqQAIuKuCvIys7HEvdsArCnpsHYLI+L4CtZhZgPINclkPLAqY+EBu2ZWLQdJABZGxDEV5GNmY4nbJF/mGqSZteYgCcBOFeRhZmOQxsBNd0c9mDwiHq2iIGZm/ajOuwCZ2WudT7fNzNpwx42ZWRdjIEhWeYMLM7MlVXSDC0mnSnpI0i2FeWtIulTSnfnv6nm+JH1L0lxJNxVvvCPpoJz+TkkHldkEB0kzq4VIvdtlphJOA2Y0zTscuCwipgGX8cpNvncFpuVpJnASpKAKHAVsA2wNHNUIrJ04SJpZPUreS7JMu2VEXEF62kHRXsDp+f/TeeUmO3sBZ0RyFbCapHWAXYBLI+LRiHiM9Ayu5sD7Km6TNLP6lG+TnCRpTuH1rIiY1eU9a0fEQoCIWChprTx/Mks+qXV+ntdufkcOkmZWn/JB8pGImF7RWltdBRgd5nc0kEFSEuNWXLHyfIeff77yPIFanpIXL71YeZ512nPyO2vJ96x5v6sl3/03fF/leQ7S0zirUvMQoAclrZNrkesAD+X584GphXRTgAV5/g5N8y/vthK3SZpZfep9fMNsoNFDfRDws8L8A3Mv97bAE/m0/GJgZ0mr5w6bnfO8jgayJmlmAyCqu3Zb0jmkWuAkSfNJvdTHAudJOhi4D9gnJ78I2A2YCzwLfBLSJdSSvg5cm9MdU+ayagdJM6tPRafbEbFfm0WvusFOflrrIW3yORU4dSTrdpA0s9r4skQzs04cJM3M2hiAZ2qX0Re925JWk3S+pNsl3SbpXb0uk5mNjqjuipte6pea5InALyPiI5KWB1budYHMbPT6PQCW0fMgKel1wHuBTwBExIvAYI2UNrPWxkCQ7IfT7TcBDwM/lHSDpJMlrdLrQplZBeodTL5M9EOQnABsCZwUEVsAz/DKLY9eJmmmpDmS5rzIC8u6jGY2UhXeBaiX+iFIzgfmR8TV+fX5pKC5hIiYFRHTI2L68qywTAtoZkvJNcnRi4gHgHmSNs2zdgL+1MMimVlFKrzpbs/0vOMm+xxwVu7Zvpt8raWZDbZ+P5Uuoy+CZETcCFR1Lzkz6wcDcCpdRl8ESTMboxwkzcxaa1xxM+gcJM2sNhoe/CjpIGlm9XCbpJlZZz7dNjPrxEGyN4IYrCfPqYYx+zFUfZ4DqI6nGgL89J4/VJ7nh6dsXXme/c41STOzThwkzczaqPBpib3kIGlmtfA4STOzbmLwo6SDpJnVxjVJM7N2PJjczKwzd9yYmXXgIGlm1k7gjhszs07ccWNm1omDpJlZax5MbmbWSYRvumtm1tHgx0gHSTOrj0+3zczaCWAMnG7XcDdYM7MsSk4lSLpH0s2SbpQ0J89bQ9Klku7Mf1fP8yXpW5LmSrpJ0pZLuwkOkmZWG0W5aQTeHxGbR8T0/Ppw4LKImAZcll8D7ApMy9NM4KSl3QYHSTOrjYaj1DQKewGn5/9PB/YuzD8jkquA1SStszQrcJA0s3qUPdVOMXKSpDmFaWabHC+RdF1h+doRsRAg/10rz58MzCu8d36eN2KD2XET1PMgMKn6PAGG/dCuukRNHQN1PLTr4gU3Vp4nwC7rbl5LvqOVBpOX/nweKZxCt7N9RCyQtBZwqaTbu6y+2VIdLK5Jmll9hktOJUTEgvz3IeBCYGvgwcZpdP77UE4+H5haePsUYMHSbIKDpJnVRhGlpq75SKtImtj4H9gZuAWYDRyUkx0E/Cz/Pxs4MPdybws80TgtH6nBPN02s/5X7Z3J1wYuVGoSmwCcHRG/lHQtcJ6kg4H7gH1y+ouA3YC5wLPAJ5d2xQ6SZlaT6q7djoi7gXe0mL8I2KnF/AAOqWLdDpJmVh/fdNfMrI3w4xvMzDobAzXJvujdlvR3km6VdIukcySt2OsymVkFKrx2u1d6HiQlTQY+D0yPiLcB44F9e1sqM6uChodLTf2sX063JwArSXoJWJmlHPRpZn0kKD1QvJ/1vCYZEfcDx5HGOC0kDfq8pDmdpJmN6zpf4oVlXUwzGyFRbiD5CC5d7ImeB8l8/7e9gA2BdYFVJO3fnC4iZkXE9IiYvhwrLOtimtnSiCg39bGeB0ngA8BfIuLhiHgJ+CmwXY/LZGZVGANBsh/aJO8DtpW0MvAcafT8nN4WycxGbYy0SfY8SEbE1ZLOB64HFgM3ALN6Wyozq0K/91yX0fMgCRARRwFH9bocZlal/j+VLqMvgqSZjUGBg6SZWUeDf7btIGlm9en3MZBlOEiaWX0cJM3M2oiAocE/33aQtCWMW2WVWvIdfuaZWvIdpCdR1vVUw3Pn/aHyPHfa7elqMnJN0sysAwdJM7M2AqjpuejLkoOkmdUkINwmaWbWWuCOGzOzjtwmaWbWgYOkmVk7vsGFmVl7AfhWaWZmHbgmaWbWji9LNDNrLyA8TtLMrANfcWNm1oHbJM3M2ohw77aZWUeuSZqZtRPE0ODc77MdB0kzq4dvlWZm1sUYGAI0rtcFMLOxKYAYjlJTGZJmSLpD0lxJh9db+lc4SJpZPSLfdLfM1IWk8cB3gV2BzYD9JG1W8xYAPt02sxpV2HGzNTA3Iu4GkHQusBfwp6pW0I5iALvoJT0M3Fsy+STgkRqLU6VBKisMVnkHqazQ+/KuHxFrjiYDSb8kbUcZKwLPF17PiohZhbw+AsyIiE/l1wcA20TEoaMpYxkDWZMcyYcnaU5ETK+zPFUZpLLCYJV3kMoKg1feViJiRoXZqdUqKsy/LbdJmtkgmA9MLbyeAixYFit2kDSzQXAtME3ShpKWB/YFZi+LFQ/k6fYIzeqepG8MUllhsMo7SGWFwStvrSJisaRDgYuB8cCpEXHrslj3QHbcWD0kDQE3k348bwMOiohnlzKvHYC/j4jdJe0JbBYRx7ZJuxrwsYj43gjXcTTwdEQctzRlNCvDp9tW9FxEbB4RbwNeBP62uFDJiI+ZiJjdLkBmqwGfHWm+ZsuCg6S181tgY0kbSLpN0veA64GpknaWdKWk6yX9RNKq8PIVEbdL+h3wPxsZSfqEpO/k/9eWdKGkP+ZpO+BYYCNJN0r6Zk73ZUnXSrpJ0j8V8vqHfNXFr4BNl9nesNcsB0l7FUkTSFc23JxnbQqcERFbAM8AXwM+EBFbAnOAwyStCPwA2AN4D/DGNtl/C/jviHgHsCVwK3A4cFeuxX5Z0s7xrXqTAAABVklEQVTANNIA4s2BrSS9V9JWpAb7LUhB+J0Vb7rZq7wWOm6svJUk3Zj//y1wCrAucG9EXJXnb0u6LOz3kgCWB64E3gz8JSLuBJB0JjCzxTp2BA4EiIgh4AlJqzel2TlPN+TXq5KC5kTgwkY7qaRl0rtpr20Oklb0XERsXpyRA+EzxVnApRGxX1O6zalucK+Af4mI/2haxxcrXIdZKT7dtpG6Cthe0sYAklaWtAlwO7ChpI1yuv3avP8y4DP5veMlvQ54ilRLbLgY+OtCW+dkSWsBVwAflrSSpImkU3uzWjlI2ohExMPAJ4BzJN1ECppvjojnSafXv8gdN+2urf8C8H5JNwPXAW+NiEWk0/dbJH0zIi4BzgauzOnOByZGxPXAj4EbgQtITQJmtfI4STOzDlyTNDPrwEHSzKwDB0kzsw4cJM3MOnCQNDPrwEHSzKwDB0kzsw7+P+SaDLzb6MnUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "rf = confusion_matrix(y_test, predict)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(rf)\n",
    "plt.title('Confusion matrix of the RandomForest Classifier')\n",
    "fig.colorbar(cax)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report highlights the precision of test data validated for RF classifier. It can be seen that youtube has higher precisio because in the selected feature values for youtube flows were distinguished by higher packet size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "class:instagram       0.94      0.97      0.95      2500\n",
      "   class:amazon       0.95      0.98      0.97      2500\n",
      "     class:bing       0.97      0.95      0.96      2500\n",
      "   class:google       0.96      0.94      0.95      2500\n",
      " class:linkedin       0.94      0.93      0.94      2500\n",
      "  class:spotify       0.96      0.96      0.96      2500\n",
      "     class:ebay       0.94      0.94      0.94      2500\n",
      " class:facebook       0.96      0.96      0.96      2500\n",
      "  class:youtube       0.98      0.97      0.98      2500\n",
      "  class:netflix       0.95      0.95      0.95      2500\n",
      "\n",
      "    avg / total       0.96      0.96      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predict, target_names=y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a roational test to observe how well our random forest classifier has been trained. We have set cv=8 to run eight iteration be choosing training data over the model. This is just an alternative report to one presented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   45.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   45.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   52.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   43.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   45.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   43.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   46.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.95010661, 0.95095949, 0.95341151, 0.95298507, 0.95368196,\n",
       "       0.95304162, 0.95336179, 0.95058698])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X_tr_st, y_train, cv=8)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network consists of many layers interconnected with nodes. Each nodes is called perceptron which is similar to multiple layer linear regression. Neural networks are good at modeling complex and non-linear relationships. This was performing much better than random forest in multi-classification. \n",
    "\n",
    "MLP is multilayer artifical nerual network that maps set of input data on appropriate outputs. Here in MLP Classifier we have used. We have utilized stochastic gradient decent 'sgd' as a solver and rectified linear unit to normalize the output. hyperbolic as a activation function for hidden layers. Because of Sgd we have implemented adapative learning rate which is modified after each epoch based on validation.Verbose is set ot true to print the progress for every iteration. Early stops helps to stop when validation is no more increasing for epochs which are set randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.32644794\n",
      "Validation score: 0.685467\n",
      "Iteration 2, loss = 0.97880287\n",
      "Validation score: 0.737333\n",
      "Iteration 3, loss = 0.89185938\n",
      "Validation score: 0.747600\n",
      "Iteration 4, loss = 0.85588384\n",
      "Validation score: 0.757333\n",
      "Iteration 5, loss = 0.82612546\n",
      "Validation score: 0.791200\n",
      "Iteration 6, loss = 0.80246099\n",
      "Validation score: 0.784667\n",
      "Iteration 7, loss = 0.78095953\n",
      "Validation score: 0.779467\n",
      "Iteration 8, loss = 0.77235801\n",
      "Validation score: 0.790933\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 9, loss = 0.65734605\n",
      "Validation score: 0.830267\n",
      "Iteration 10, loss = 0.60232932\n",
      "Validation score: 0.834800\n",
      "Iteration 11, loss = 0.58168781\n",
      "Validation score: 0.838400\n",
      "Iteration 12, loss = 0.56749035\n",
      "Validation score: 0.838133\n",
      "Iteration 13, loss = 0.55665096\n",
      "Validation score: 0.838667\n",
      "Iteration 14, loss = 0.54770297\n",
      "Validation score: 0.843200\n",
      "Iteration 15, loss = 0.54153026\n",
      "Validation score: 0.844267\n",
      "Iteration 16, loss = 0.53582723\n",
      "Validation score: 0.843467\n",
      "Iteration 17, loss = 0.52922366\n",
      "Validation score: 0.845200\n",
      "Iteration 18, loss = 0.52639228\n",
      "Validation score: 0.849200\n",
      "Iteration 19, loss = 0.51969432\n",
      "Validation score: 0.850667\n",
      "Iteration 20, loss = 0.51603586\n",
      "Validation score: 0.845600\n",
      "Iteration 21, loss = 0.51398236\n",
      "Validation score: 0.849467\n",
      "Iteration 22, loss = 0.50897822\n",
      "Validation score: 0.844533\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 23, loss = 0.47512163\n",
      "Validation score: 0.860000\n",
      "Iteration 24, loss = 0.46006563\n",
      "Validation score: 0.861200\n",
      "Iteration 25, loss = 0.45647771\n",
      "Validation score: 0.862000\n",
      "Iteration 26, loss = 0.45385018\n",
      "Validation score: 0.863067\n",
      "Iteration 27, loss = 0.45183418\n",
      "Validation score: 0.862667\n",
      "Iteration 28, loss = 0.45007918\n",
      "Validation score: 0.863200\n",
      "Iteration 29, loss = 0.44782409\n",
      "Validation score: 0.864533\n",
      "Iteration 30, loss = 0.44666482\n",
      "Validation score: 0.860800\n",
      "Iteration 31, loss = 0.44520558\n",
      "Validation score: 0.863467\n",
      "Iteration 32, loss = 0.44354681\n",
      "Validation score: 0.863867\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 33, loss = 0.43614082\n",
      "Validation score: 0.866267\n",
      "Iteration 34, loss = 0.43425329\n",
      "Validation score: 0.865333\n",
      "Iteration 35, loss = 0.43355801\n",
      "Validation score: 0.865333\n",
      "Iteration 36, loss = 0.43335217\n",
      "Validation score: 0.866800\n",
      "Iteration 37, loss = 0.43297143\n",
      "Validation score: 0.865333\n",
      "Iteration 38, loss = 0.43266186\n",
      "Validation score: 0.866933\n",
      "Iteration 39, loss = 0.43238644\n",
      "Validation score: 0.867200\n",
      "Iteration 40, loss = 0.43207777\n",
      "Validation score: 0.867467\n",
      "Iteration 41, loss = 0.43182356\n",
      "Validation score: 0.866133\n",
      "Iteration 42, loss = 0.43146748\n",
      "Validation score: 0.866933\n",
      "Iteration 43, loss = 0.43135525\n",
      "Validation score: 0.867467\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 44, loss = 0.42927410\n",
      "Validation score: 0.866933\n",
      "Iteration 45, loss = 0.42909724\n",
      "Validation score: 0.867600\n",
      "Iteration 46, loss = 0.42897336\n",
      "Validation score: 0.867467\n",
      "Iteration 47, loss = 0.42893006\n",
      "Validation score: 0.868000\n",
      "Iteration 48, loss = 0.42885312\n",
      "Validation score: 0.867333\n",
      "Iteration 49, loss = 0.42879080\n",
      "Validation score: 0.867200\n",
      "Iteration 50, loss = 0.42875040\n",
      "Validation score: 0.868133\n",
      "Iteration 51, loss = 0.42861966\n",
      "Validation score: 0.868667\n",
      "Iteration 52, loss = 0.42861736\n",
      "Validation score: 0.866933\n",
      "Iteration 53, loss = 0.42853737\n",
      "Validation score: 0.867867\n",
      "Iteration 54, loss = 0.42847343\n",
      "Validation score: 0.867600\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 55, loss = 0.42805611\n",
      "Validation score: 0.867867\n",
      "Iteration 56, loss = 0.42801366\n",
      "Validation score: 0.868000\n",
      "Iteration 57, loss = 0.42800496\n",
      "Validation score: 0.868000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 58, loss = 0.42790437\n",
      "Validation score: 0.868000\n",
      "Iteration 59, loss = 0.42790104\n",
      "Validation score: 0.868000\n",
      "Iteration 60, loss = 0.42789733\n",
      "Validation score: 0.868000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 61, loss = 0.42787581\n",
      "Validation score: 0.868000\n",
      "Iteration 62, loss = 0.42787520\n",
      "Validation score: 0.868000\n",
      "Iteration 63, loss = 0.42787455\n",
      "Validation score: 0.868000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 64, loss = 0.42787019\n",
      "Validation score: 0.868000\n",
      "Iteration 65, loss = 0.42787005\n",
      "Validation score: 0.868000\n",
      "Iteration 66, loss = 0.42786994\n",
      "Validation score: 0.868000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "NN = MLPClassifier( activation='relu',solver='sgd',learning_rate='adaptive',\n",
    "                   hidden_layer_sizes=(100,200,100), learning_rate_init=0.1, alpha=0.1,\n",
    "                    shuffle=True, verbose=True,early_stopping=True)\n",
    "model = NN.fit(X_tr_st, y_train)\n",
    "NN_predicted = model.predict(X_test_st);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Shows accuracy for Neural Network which is typically lower than Random Forest. Neural Networks often covers local minima not global minima, which often means it misses the total picture and it catches overfitting problem in general but in our case we have applied early stopping to avoid it. Neural network is computational costly as it took more time to train than RF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87492"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accr = metrics.accuracy_score(y_test, NN_predicted)\n",
    "accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "class:instagram       0.94      0.97      0.95      2500\n",
      "   class:amazon       0.95      0.98      0.97      2500\n",
      "     class:bing       0.97      0.95      0.96      2500\n",
      "   class:google       0.96      0.94      0.95      2500\n",
      " class:linkedin       0.94      0.93      0.94      2500\n",
      "  class:spotify       0.96      0.96      0.96      2500\n",
      "     class:ebay       0.94      0.94      0.94      2500\n",
      " class:facebook       0.96      0.96      0.96      2500\n",
      "  class:youtube       0.98      0.97      0.98      2500\n",
      "  class:netflix       0.95      0.95      0.95      2500\n",
      "\n",
      "    avg / total       0.96      0.96      0.96     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predict, target_names=y_test.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overfitting, we try to use the normalization factor alpha which penalizes weights with big magnitude in L2 equalization. Validtion score with each iteration is show during the training phase.Whenever there is no significant improvement in precision our adapative learning rate varies and tries the new epoch with smaller learning rate to avoid under/overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porblem of missing forest for trees is typically covered by Neural Network which has some other problems of slow computation and gradient. Random Forest is trainined seperately for trees which results as an ensemble of decision trees where as NN works as a single model where we use backpropagation to reduce error from every layer. One important thing about neural networks is that they are really diverse. Most random forests are applied to some set of predictor variables that don't necessarily have any kind of important arrangement\n",
    "\n",
    "In our case where we have ten classes of different traffic, RF will do it well for each class resulting in some accuracy for resemblance with a class and we can classify the flow to highest scoring trees. Problem in this case is choosing a higher estimators for proper validaiton score. for kind of data where as Neural Networks are universal approximators works better for forecasting and Image processing problems. \n",
    "\n",
    "In general,A tree is easy to create, is able to handle different types of input data (categorical and\n",
    "numerical) and is interpretable due to its kind of representation. However, Decision Trees often show\n",
    "a lack of reliability in their application to new data. Random Forest Classifiers typically work faster with tabular data. An important strength of Random Forests is that they are able to perform still well in the case of missing data. According to their construction principle, not every tree is using the same features. According to the results, Random Forest is clearly the best classifier as it achieves the best classification results \n",
    "\n",
    "Inspite of all these benefits it may be concluded that best classifier can only be declared based on input data, NN perform well in Image classification and Speech recognition.\n",
    "\n",
    "\n",
    "Authored by : Neel Kanwal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
